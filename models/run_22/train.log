===============================================================================                                                            [259/1939]

Train file         = splits/train_5.7k_cropped_remapped_siamese.txt
Val file           = splits/val_5.7k_cropped_remapped.txt
Run No             = 22
Model save folder  = models/run_22
-------------------------------
Optimisation Parameters
-------------------------------
lr                 = 0.00050
epochs             = 100
Batch size         = 48
Test Batch size    = 16
Using device: cuda
THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=405 error=11 : invalid argument
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 112, 112]           9,408
       BatchNorm2d-2         [-1, 64, 112, 112]             128
              ReLU-3         [-1, 64, 112, 112]               0
         MaxPool2d-4           [-1, 64, 56, 56]               0
            Conv2d-5           [-1, 64, 56, 56]          36,864
       BatchNorm2d-6           [-1, 64, 56, 56]             128
              ReLU-7           [-1, 64, 56, 56]               0
            Conv2d-8           [-1, 64, 56, 56]          36,864
       BatchNorm2d-9           [-1, 64, 56, 56]             128
             ReLU-10           [-1, 64, 56, 56]               0
       BasicBlock-11           [-1, 64, 56, 56]               0
           Conv2d-12           [-1, 64, 56, 56]          36,864
      BatchNorm2d-13           [-1, 64, 56, 56]             128
             ReLU-14           [-1, 64, 56, 56]               0
           Conv2d-15           [-1, 64, 56, 56]          36,864
      BatchNorm2d-16           [-1, 64, 56, 56]             128
             ReLU-17           [-1, 64, 56, 56]               0
       BasicBlock-18           [-1, 64, 56, 56]               0
           Conv2d-19          [-1, 128, 28, 28]          73,728                                                                            [223/1939]
      BatchNorm2d-20          [-1, 128, 28, 28]             256
             ReLU-21          [-1, 128, 28, 28]               0
           Conv2d-22          [-1, 128, 28, 28]         147,456
      BatchNorm2d-23          [-1, 128, 28, 28]             256
           Conv2d-24          [-1, 128, 28, 28]           8,192
      BatchNorm2d-25          [-1, 128, 28, 28]             256
             ReLU-26          [-1, 128, 28, 28]               0
       BasicBlock-27          [-1, 128, 28, 28]               0
           Conv2d-28          [-1, 128, 28, 28]         147,456
      BatchNorm2d-29          [-1, 128, 28, 28]             256
             ReLU-30          [-1, 128, 28, 28]               0
           Conv2d-31          [-1, 128, 28, 28]         147,456
      BatchNorm2d-32          [-1, 128, 28, 28]             256
             ReLU-33          [-1, 128, 28, 28]               0
       BasicBlock-34          [-1, 128, 28, 28]               0
           Conv2d-35          [-1, 256, 14, 14]         294,912
      BatchNorm2d-36          [-1, 256, 14, 14]             512
             ReLU-37          [-1, 256, 14, 14]               0
           Conv2d-38          [-1, 256, 14, 14]         589,824
      BatchNorm2d-39          [-1, 256, 14, 14]             512
           Conv2d-40          [-1, 256, 14, 14]          32,768
      BatchNorm2d-41          [-1, 256, 14, 14]             512
             ReLU-42          [-1, 256, 14, 14]               0
       BasicBlock-43          [-1, 256, 14, 14]               0
           Conv2d-44          [-1, 256, 14, 14]         589,824
      BatchNorm2d-45          [-1, 256, 14, 14]             512
             ReLU-46          [-1, 256, 14, 14]               0
           Conv2d-47          [-1, 256, 14, 14]         589,824
      BatchNorm2d-48          [-1, 256, 14, 14]             512
             ReLU-49          [-1, 256, 14, 14]               0
       BasicBlock-50          [-1, 256, 14, 14]               0
           Conv2d-51            [-1, 512, 7, 7]       1,179,648
      BatchNorm2d-52            [-1, 512, 7, 7]           1,024
             ReLU-53            [-1, 512, 7, 7]               0
           Conv2d-54            [-1, 512, 7, 7]       2,359,296
      BatchNorm2d-55            [-1, 512, 7, 7]           1,024                                                                            [187/1939]
           Conv2d-56            [-1, 512, 7, 7]         131,072
      BatchNorm2d-57            [-1, 512, 7, 7]           1,024
             ReLU-58            [-1, 512, 7, 7]               0
       BasicBlock-59            [-1, 512, 7, 7]               0
           Conv2d-60            [-1, 512, 7, 7]       2,359,296
      BatchNorm2d-61            [-1, 512, 7, 7]           1,024
             ReLU-62            [-1, 512, 7, 7]               0
           Conv2d-63            [-1, 512, 7, 7]       2,359,296
      BatchNorm2d-64            [-1, 512, 7, 7]           1,024
             ReLU-65            [-1, 512, 7, 7]               0
       BasicBlock-66            [-1, 512, 7, 7]               0
        MaxPool2d-67            [-1, 512, 1, 1]               0
          Flatten-68                  [-1, 512]               0
           ResNet-69                  [-1, 512]               0
           Conv2d-70         [-1, 64, 112, 112]           9,408
      BatchNorm2d-71         [-1, 64, 112, 112]             128
             ReLU-72         [-1, 64, 112, 112]               0
        MaxPool2d-73           [-1, 64, 56, 56]               0
           Conv2d-74           [-1, 64, 56, 56]          36,864
      BatchNorm2d-75           [-1, 64, 56, 56]             128
             ReLU-76           [-1, 64, 56, 56]               0
           Conv2d-77           [-1, 64, 56, 56]          36,864
      BatchNorm2d-78           [-1, 64, 56, 56]             128
             ReLU-79           [-1, 64, 56, 56]               0
       BasicBlock-80           [-1, 64, 56, 56]               0
           Conv2d-81           [-1, 64, 56, 56]          36,864
      BatchNorm2d-82           [-1, 64, 56, 56]             128
             ReLU-83           [-1, 64, 56, 56]               0
           Conv2d-84           [-1, 64, 56, 56]          36,864
      BatchNorm2d-85           [-1, 64, 56, 56]             128
             ReLU-86           [-1, 64, 56, 56]               0
       BasicBlock-87           [-1, 64, 56, 56]               0
           Conv2d-88          [-1, 128, 28, 28]          73,728
      BatchNorm2d-89          [-1, 128, 28, 28]             256
             ReLU-90          [-1, 128, 28, 28]               0
           Conv2d-91          [-1, 128, 28, 28]         147,456                                                                            [151/1939]
      BatchNorm2d-92          [-1, 128, 28, 28]             256
           Conv2d-93          [-1, 128, 28, 28]           8,192
      BatchNorm2d-94          [-1, 128, 28, 28]             256
             ReLU-95          [-1, 128, 28, 28]               0
       BasicBlock-96          [-1, 128, 28, 28]               0
           Conv2d-97          [-1, 128, 28, 28]         147,456
      BatchNorm2d-98          [-1, 128, 28, 28]             256
             ReLU-99          [-1, 128, 28, 28]               0
          Conv2d-100          [-1, 128, 28, 28]         147,456
     BatchNorm2d-101          [-1, 128, 28, 28]             256
            ReLU-102          [-1, 128, 28, 28]               0
      BasicBlock-103          [-1, 128, 28, 28]               0
          Conv2d-104          [-1, 256, 14, 14]         294,912
     BatchNorm2d-105          [-1, 256, 14, 14]             512
            ReLU-106          [-1, 256, 14, 14]               0
          Conv2d-107          [-1, 256, 14, 14]         589,824
     BatchNorm2d-108          [-1, 256, 14, 14]             512
          Conv2d-109          [-1, 256, 14, 14]          32,768
     BatchNorm2d-110          [-1, 256, 14, 14]             512
            ReLU-111          [-1, 256, 14, 14]               0
      BasicBlock-112          [-1, 256, 14, 14]               0
          Conv2d-113          [-1, 256, 14, 14]         589,824
     BatchNorm2d-114          [-1, 256, 14, 14]             512
            ReLU-115          [-1, 256, 14, 14]               0
          Conv2d-116          [-1, 256, 14, 14]         589,824
     BatchNorm2d-117          [-1, 256, 14, 14]             512
            ReLU-118          [-1, 256, 14, 14]               0
      BasicBlock-119          [-1, 256, 14, 14]               0
          Conv2d-120            [-1, 512, 7, 7]       1,179,648
     BatchNorm2d-121            [-1, 512, 7, 7]           1,024
            ReLU-122            [-1, 512, 7, 7]               0
          Conv2d-123            [-1, 512, 7, 7]       2,359,296
     BatchNorm2d-124            [-1, 512, 7, 7]           1,024
          Conv2d-125            [-1, 512, 7, 7]         131,072
     BatchNorm2d-126            [-1, 512, 7, 7]           1,024
            ReLU-127            [-1, 512, 7, 7]               0                                                                            [115/1939]
      BasicBlock-128            [-1, 512, 7, 7]               0
          Conv2d-129            [-1, 512, 7, 7]       2,359,296
     BatchNorm2d-130            [-1, 512, 7, 7]           1,024
            ReLU-131            [-1, 512, 7, 7]               0
          Conv2d-132            [-1, 512, 7, 7]       2,359,296
     BatchNorm2d-133            [-1, 512, 7, 7]           1,024
            ReLU-134            [-1, 512, 7, 7]               0
      BasicBlock-135            [-1, 512, 7, 7]               0
       MaxPool2d-136            [-1, 512, 1, 1]               0
         Flatten-137                  [-1, 512]               0
          ResNet-138                  [-1, 512]               0
================================================================
Total params: 22,353,024
Trainable params: 22,353,024
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 86436.00
Forward/backward pass size (MB): 125.59
Params size (MB): 85.27
Estimated Total Size (MB): 86646.86
----------------------------------------------------------------
Train images done
Val images done
Epoch: 1 Training_Loss: 0.135583 Val_acc: 0.246951
Train images done
Val images done
Epoch: 2 Training_Loss: 0.101831 Val_acc: 0.280488
Train images done
Val images done
Epoch: 3 Training_Loss: 0.088387 Val_acc: 0.265244
Train images done
Val images done
Epoch: 4 Training_Loss: 0.078483 Val_acc: 0.250000
Train images done
Val images done
Epoch: 5 Training_Loss: 0.071293 Val_acc: 0.243902                                                                                          [79/1939]
Train images done
Val images done
Epoch: 6 Training_Loss: 0.063739 Val_acc: 0.260671
Train images done
Val images done
Epoch: 7 Training_Loss: 0.057009 Val_acc: 0.234756
Train images done
Val images done
Epoch: 8 Training_Loss: 0.052354 Val_acc: 0.236280
Train images done
Val images done
Epoch: 9 Training_Loss: 0.046825 Val_acc: 0.239329
Train images done
Val images done
Epoch: 10 Training_Loss: 0.043819 Val_acc: 0.236280
Train images done
Val images done
Epoch: 11 Training_Loss: 0.039474 Val_acc: 0.214939
Train images done
Val images done
Epoch: 12 Training_Loss: 0.036334 Val_acc: 0.239329
Train images done
Val images done
Epoch: 13 Training_Loss: 0.034169 Val_acc: 0.216463
Train images done
Val images done
Epoch: 14 Training_Loss: 0.031228 Val_acc: 0.228659
Train images done
Val images done
Epoch: 15 Training_Loss: 0.028624 Val_acc: 0.236280
Train images done
Val images done
Epoch: 16 Training_Loss: 0.025675 Val_acc: 0.211890
Train images done
Val images done
Epoch: 17 Training_Loss: 0.023797 Val_acc: 0.219512                                                                                         [43/1939]
Train images done
Val images done
Epoch: 18 Training_Loss: 0.022624 Val_acc: 0.224085
Train images done
Val images done
Epoch: 19 Training_Loss: 0.021173 Val_acc: 0.221037
Train images done
Val images done
Epoch: 20 Training_Loss: 0.020987 Val_acc: 0.219512
Train images done
Val images done
Epoch: 21 Training_Loss: 0.019113 Val_acc: 0.221037
Train images done
Val images done
Epoch: 22 Training_Loss: 0.017844 Val_acc: 0.207317
Train images done
Val images done
Epoch: 23 Training_Loss: 0.016655 Val_acc: 0.227134
Train images done
Val images done
Epoch: 24 Training_Loss: 0.015816 Val_acc: 0.211890
Train images done
Val images done
Epoch: 25 Training_Loss: 0.014871 Val_acc: 0.213415
Train images done
Val images done
Epoch: 26 Training_Loss: 0.015266 Val_acc: 0.217988
Train images done
Val images done
Epoch: 27 Training_Loss: 0.013421 Val_acc: 0.205793
Train images done
Val images done
Epoch: 28 Training_Loss: 0.012667 Val_acc: 0.228659
Train images done
Val images done
Epoch: 29 Training_Loss: 0.012780 Val_acc: 0.208841                                                                                          [7/1939]
Train images done
Val images done
Epoch: 30 Training_Loss: 0.012516 Val_acc: 0.210366
Train images done
Val images done
Epoch: 31 Training_Loss: 0.012152 Val_acc: 0.222561
Train images done
Val images done
Epoch: 32 Training_Loss: 0.011780 Val_acc: 0.221037
Train images done
Val images done
Epoch: 33 Training_Loss: 0.011346 Val_acc: 0.210366
Train images done
Val images done
Epoch: 34 Training_Loss: 0.012588 Val_acc: 0.222561
Train images done
Val images done
Epoch: 35 Training_Loss: 0.009601 Val_acc: 0.205793
Train images done
Val images done
Epoch: 36 Training_Loss: 0.012268 Val_acc: 0.211890
Train images done
Val images done
Epoch: 37 Training_Loss: 0.017546 Val_acc: 0.205793
Train images done
Val images done
Epoch: 38 Training_Loss: 0.010327 Val_acc: 0.213415
Train images done
Val images done
Epoch: 39 Training_Loss: 0.007550 Val_acc: 0.210366
Train images done
Val images done
Epoch: 40 Training_Loss: 0.006263 Val_acc: 0.208841
