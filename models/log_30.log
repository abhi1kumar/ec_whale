===============================================================================

Train file         = splits/train_5.7k_cropped_remapped_siamese_latest.txt
Val file           = splits/val_5.7k_cropped_remapped.txt
Run No             = 30
Model save folder  = models/run_30
-------------------------------
Optimisation Parameters
-------------------------------
lr                 = 0.00050
epochs             = 100
Batch size         = 48
Test Batch size    = 16
Using device: cuda
THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=405 error=11 : invalid argument
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 112, 112]           9,408
       BatchNorm2d-2         [-1, 64, 112, 112]             128
              ReLU-3         [-1, 64, 112, 112]               0
         MaxPool2d-4           [-1, 64, 56, 56]               0
            Conv2d-5           [-1, 64, 56, 56]          36,864
       BatchNorm2d-6           [-1, 64, 56, 56]             128
              ReLU-7           [-1, 64, 56, 56]               0
            Conv2d-8           [-1, 64, 56, 56]          36,864
       BatchNorm2d-9           [-1, 64, 56, 56]             128
             ReLU-10           [-1, 64, 56, 56]               0                                                                            [260/1865]
       BasicBlock-11           [-1, 64, 56, 56]               0
           Conv2d-12           [-1, 64, 56, 56]          36,864
      BatchNorm2d-13           [-1, 64, 56, 56]             128
             ReLU-14           [-1, 64, 56, 56]               0
           Conv2d-15           [-1, 64, 56, 56]          36,864
      BatchNorm2d-16           [-1, 64, 56, 56]             128
             ReLU-17           [-1, 64, 56, 56]               0
       BasicBlock-18           [-1, 64, 56, 56]               0
           Conv2d-19          [-1, 128, 28, 28]          73,728
      BatchNorm2d-20          [-1, 128, 28, 28]             256
             ReLU-21          [-1, 128, 28, 28]               0
           Conv2d-22          [-1, 128, 28, 28]         147,456
      BatchNorm2d-23          [-1, 128, 28, 28]             256
           Conv2d-24          [-1, 128, 28, 28]           8,192
      BatchNorm2d-25          [-1, 128, 28, 28]             256
             ReLU-26          [-1, 128, 28, 28]               0
       BasicBlock-27          [-1, 128, 28, 28]               0
           Conv2d-28          [-1, 128, 28, 28]         147,456
      BatchNorm2d-29          [-1, 128, 28, 28]             256
             ReLU-30          [-1, 128, 28, 28]               0
           Conv2d-31          [-1, 128, 28, 28]         147,456
      BatchNorm2d-32          [-1, 128, 28, 28]             256
             ReLU-33          [-1, 128, 28, 28]               0
       BasicBlock-34          [-1, 128, 28, 28]               0
           Conv2d-35          [-1, 256, 14, 14]         294,912
      BatchNorm2d-36          [-1, 256, 14, 14]             512
             ReLU-37          [-1, 256, 14, 14]               0
           Conv2d-38          [-1, 256, 14, 14]         589,824
      BatchNorm2d-39          [-1, 256, 14, 14]             512
           Conv2d-40          [-1, 256, 14, 14]          32,768
      BatchNorm2d-41          [-1, 256, 14, 14]             512
             ReLU-42          [-1, 256, 14, 14]               0
       BasicBlock-43          [-1, 256, 14, 14]               0
           Conv2d-44          [-1, 256, 14, 14]         589,824
      BatchNorm2d-45          [-1, 256, 14, 14]             512
             ReLU-46          [-1, 256, 14, 14]               0                                                                            [224/1865]
           Conv2d-47          [-1, 256, 14, 14]         589,824
      BatchNorm2d-48          [-1, 256, 14, 14]             512
             ReLU-49          [-1, 256, 14, 14]               0
       BasicBlock-50          [-1, 256, 14, 14]               0
           Conv2d-51            [-1, 512, 7, 7]       1,179,648
      BatchNorm2d-52            [-1, 512, 7, 7]           1,024
             ReLU-53            [-1, 512, 7, 7]               0
           Conv2d-54            [-1, 512, 7, 7]       2,359,296
      BatchNorm2d-55            [-1, 512, 7, 7]           1,024
           Conv2d-56            [-1, 512, 7, 7]         131,072
      BatchNorm2d-57            [-1, 512, 7, 7]           1,024
             ReLU-58            [-1, 512, 7, 7]               0
       BasicBlock-59            [-1, 512, 7, 7]               0
           Conv2d-60            [-1, 512, 7, 7]       2,359,296
      BatchNorm2d-61            [-1, 512, 7, 7]           1,024
             ReLU-62            [-1, 512, 7, 7]               0
           Conv2d-63            [-1, 512, 7, 7]       2,359,296
      BatchNorm2d-64            [-1, 512, 7, 7]           1,024
             ReLU-65            [-1, 512, 7, 7]               0
       BasicBlock-66            [-1, 512, 7, 7]               0
        MaxPool2d-67            [-1, 512, 1, 1]               0
          Flatten-68                  [-1, 512]               0
           ResNet-69                  [-1, 512]               0
           Conv2d-70         [-1, 64, 112, 112]           9,408
      BatchNorm2d-71         [-1, 64, 112, 112]             128
             ReLU-72         [-1, 64, 112, 112]               0
        MaxPool2d-73           [-1, 64, 56, 56]               0
           Conv2d-74           [-1, 64, 56, 56]          36,864
      BatchNorm2d-75           [-1, 64, 56, 56]             128
             ReLU-76           [-1, 64, 56, 56]               0
           Conv2d-77           [-1, 64, 56, 56]          36,864
      BatchNorm2d-78           [-1, 64, 56, 56]             128
             ReLU-79           [-1, 64, 56, 56]               0
       BasicBlock-80           [-1, 64, 56, 56]               0
           Conv2d-81           [-1, 64, 56, 56]          36,864
      BatchNorm2d-82           [-1, 64, 56, 56]             128                                                                            [188/1865]
             ReLU-83           [-1, 64, 56, 56]               0
           Conv2d-84           [-1, 64, 56, 56]          36,864
      BatchNorm2d-85           [-1, 64, 56, 56]             128
             ReLU-86           [-1, 64, 56, 56]               0
       BasicBlock-87           [-1, 64, 56, 56]               0
           Conv2d-88          [-1, 128, 28, 28]          73,728
      BatchNorm2d-89          [-1, 128, 28, 28]             256
             ReLU-90          [-1, 128, 28, 28]               0
           Conv2d-91          [-1, 128, 28, 28]         147,456
      BatchNorm2d-92          [-1, 128, 28, 28]             256
           Conv2d-93          [-1, 128, 28, 28]           8,192
      BatchNorm2d-94          [-1, 128, 28, 28]             256
             ReLU-95          [-1, 128, 28, 28]               0
       BasicBlock-96          [-1, 128, 28, 28]               0
           Conv2d-97          [-1, 128, 28, 28]         147,456
      BatchNorm2d-98          [-1, 128, 28, 28]             256
             ReLU-99          [-1, 128, 28, 28]               0
          Conv2d-100          [-1, 128, 28, 28]         147,456
     BatchNorm2d-101          [-1, 128, 28, 28]             256
            ReLU-102          [-1, 128, 28, 28]               0
      BasicBlock-103          [-1, 128, 28, 28]               0
          Conv2d-104          [-1, 256, 14, 14]         294,912
     BatchNorm2d-105          [-1, 256, 14, 14]             512
            ReLU-106          [-1, 256, 14, 14]               0
          Conv2d-107          [-1, 256, 14, 14]         589,824
     BatchNorm2d-108          [-1, 256, 14, 14]             512
          Conv2d-109          [-1, 256, 14, 14]          32,768
     BatchNorm2d-110          [-1, 256, 14, 14]             512
            ReLU-111          [-1, 256, 14, 14]               0
      BasicBlock-112          [-1, 256, 14, 14]               0
          Conv2d-113          [-1, 256, 14, 14]         589,824
     BatchNorm2d-114          [-1, 256, 14, 14]             512
            ReLU-115          [-1, 256, 14, 14]               0
          Conv2d-116          [-1, 256, 14, 14]         589,824
     BatchNorm2d-117          [-1, 256, 14, 14]             512
            ReLU-118          [-1, 256, 14, 14]               0                                                                            [152/1865]
      BasicBlock-119          [-1, 256, 14, 14]               0
          Conv2d-120            [-1, 512, 7, 7]       1,179,648
     BatchNorm2d-121            [-1, 512, 7, 7]           1,024
            ReLU-122            [-1, 512, 7, 7]               0
          Conv2d-123            [-1, 512, 7, 7]       2,359,296
     BatchNorm2d-124            [-1, 512, 7, 7]           1,024
          Conv2d-125            [-1, 512, 7, 7]         131,072
     BatchNorm2d-126            [-1, 512, 7, 7]           1,024
            ReLU-127            [-1, 512, 7, 7]               0
      BasicBlock-128            [-1, 512, 7, 7]               0
          Conv2d-129            [-1, 512, 7, 7]       2,359,296
     BatchNorm2d-130            [-1, 512, 7, 7]           1,024
            ReLU-131            [-1, 512, 7, 7]               0
          Conv2d-132            [-1, 512, 7, 7]       2,359,296
     BatchNorm2d-133            [-1, 512, 7, 7]           1,024
            ReLU-134            [-1, 512, 7, 7]               0
      BasicBlock-135            [-1, 512, 7, 7]               0
       MaxPool2d-136            [-1, 512, 1, 1]               0
         Flatten-137                  [-1, 512]               0
          ResNet-138                  [-1, 512]               0
================================================================
Total params: 22,353,024
Trainable params: 22,353,024
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 86436.00
Forward/backward pass size (MB): 125.59
Params size (MB): 85.27
Estimated Total Size (MB): 86646.86
----------------------------------------------------------------
Train images done
Val images done
Epoch: 1 Training_Loss: 0.061914 Val_acc: 0.266768
Train images done
Val images don
Epoch: 2 Training_Loss: 0.033030 Val_acc: 0.288110                                                                                         [116/1865]
Train images done
Val images done
Epoch: 3 Training_Loss: 0.024683 Val_acc: 0.278963
Train images done
Val images done
Epoch: 4 Training_Loss: 0.020111 Val_acc: 0.289634
Train images done
Val images done
Epoch: 5 Training_Loss: 0.016781 Val_acc: 0.291159
Train images done
Val images done
Epoch: 6 Training_Loss: 0.014408 Val_acc: 0.292683
Train images done
Val images done
Epoch: 7 Training_Loss: 0.012470 Val_acc: 0.300305
Train images done
Val images done
Epoch: 8 Training_Loss: 0.011047 Val_acc: 0.303354
Train images done
Val images done
Epoch: 9 Training_Loss: 0.010019 Val_acc: 0.286585
Train images done
Val images done
Epoch: 10 Training_Loss: 0.009042 Val_acc: 0.288110
Train images done
Val images done
Epoch: 11 Training_Loss: 0.008135 Val_acc: 0.294207
Train images done
Val images done
Epoch: 12 Training_Loss: 0.007397 Val_acc: 0.285061
Train images done
Val images done
Epoch: 13 Training_Loss: 0.006675 Val_acc: 0.303354
Train images done
Val images done
Epoch: 14 Training_Loss: 0.006318 Val_acc: 0.289634                                                                                         [80/1865]
Train images done
Val images done
Epoch: 15 Training_Loss: 0.005779 Val_acc: 0.303354
Train images done
Val images done
Epoch: 16 Training_Loss: 0.005162 Val_acc: 0.283537
Train images done
Val images done
Epoch: 17 Training_Loss: 0.005087 Val_acc: 0.303354
Train images done
Val images done
Epoch: 18 Training_Loss: 0.004442 Val_acc: 0.288110
Train images done
Val images done
Epoch: 19 Training_Loss: 0.004287 Val_acc: 0.291159
Train images done
Val images done
Epoch: 20 Training_Loss: 0.004108 Val_acc: 0.304878
Train images done
Val images done
Epoch: 21 Training_Loss: 0.004140 Val_acc: 0.289634
Train images done
Val images done
Epoch: 22 Training_Loss: 0.003500 Val_acc: 0.295732
Train images done
Val images done
Epoch: 23 Training_Loss: 0.003504 Val_acc: 0.295732
Train images done
Val images done
Epoch: 24 Training_Loss: 0.003188 Val_acc: 0.288110
Train images done
Val images done
Epoch: 25 Training_Loss: 0.003039 Val_acc: 0.283537
Train images done
Val images done
Epoch: 26 Training_Loss: 0.002979 Val_acc: 0.303354                                                                                         [44/1865]
Train images done
Val images done
Epoch: 27 Training_Loss: 0.002614 Val_acc: 0.289634
Train images done
Val images done
Epoch: 28 Training_Loss: 0.002546 Val_acc: 0.309451
Train images done
Val images done
Epoch: 29 Training_Loss: 0.002537 Val_acc: 0.309451
Train images done
Val images done
Epoch: 30 Training_Loss: 0.002457 Val_acc: 0.292683
Train images done
Val images done
Epoch: 31 Training_Loss: 0.002148 Val_acc: 0.289634
Train images done
Val images done
Epoch: 32 Training_Loss: 0.002299 Val_acc: 0.292683
Train images done
Val images done
Epoch: 33 Training_Loss: 0.002077 Val_acc: 0.292683
Train images done
Val images done
Epoch: 34 Training_Loss: 0.002251 Val_acc: 0.295732
Train images done
Val images done
Epoch: 35 Training_Loss: 0.002044 Val_acc: 0.307927
Train images done
Val images done
Epoch: 36 Training_Loss: 0.001914 Val_acc: 0.288110
Train images done
Val images done
Epoch: 37 Training_Loss: 0.001859 Val_acc: 0.310976
Train images done
Val images done
Epoch: 38 Training_Loss: 0.001992 Val_acc: 0.298780                                                                                          [8/1865]
Train images done
Val images done
Epoch: 39 Training_Loss: 0.001531 Val_acc: 0.291159
Train images done
Val images done
Epoch: 40 Training_Loss: 0.001815 Val_acc: 0.297256
Train images done
Val images done
Epoch: 41 Training_Loss: 0.001600 Val_acc: 0.303354
Train images done
Val images done
Epoch: 42 Training_Loss: 0.001510 Val_acc: 0.310976
Train images done
Val images done
Epoch: 43 Training_Loss: 0.001632 Val_acc: 0.285061
Train images done
Val images done
Epoch: 44 Training_Loss: 0.001511 Val_acc: 0.309451
Train images done
Val images done
Epoch: 45 Training_Loss: 0.001337 Val_acc: 0.306402
Train images done
Val images done
Epoch: 46 Training_Loss: 0.001374 Val_acc: 0.298780
Train images done
Val images done
Epoch: 47 Training_Loss: 0.001237 Val_acc: 0.300305
Train images done
Val images done
Epoch: 48 Training_Loss: 0.001163 Val_acc: 0.295732
Train images done
Val images done
Epoch: 49 Training_Loss: 0.001334 Val_acc: 0.295732
Train images done
Val images done
Epoch: 50 Training_Loss: 0.001204 Val_acc: 0.294207
